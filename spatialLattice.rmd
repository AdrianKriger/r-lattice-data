---
##-- arkriger: October 2023
title: "Exploring Lattice Data with `R`"
output:
  html_notebook: default
  html_document:
    df_print: paged
---
<style> 
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}

</style>
<div class="alert alert-info"> <strong>NAME: </strong> _[write your name.surname between the brackets (like that name.surname)]_

</div>  

Welcome to this exercise which explores **_Lattice Data_**. 

We we talk about lattice data we mean: observations from a random process sampled over a countable collection of spatial regions, and supplemented by a neighborhood structure. The observation locations can be regular (equally spaced grid) or irregular, and data at a particular location typically represent the entire region. _i.e.: census data is lattice data._

This assignment is a brief tutorial on the _**spatial regression**_ functionality contained in the `R` package `spdep`. We will start to learn some of the methods necessary to determine _**spatial autocorrelation**_ and later continue onto the `spgwr` package which explores _**geographically weighted regression**_.

<div class="alert alert-block alert-success"> **Our focus is understanding concepts, the analysis, potential and application of these methods.** </div>

We do not cover all the details of the `spdep` and `spgwr` packages. If you are unsure what the parameters for a particular function are or if you wish to explore other arguments related to a function, please consult the `help` files or the relevant package vignettes.

In this assignment, we will work through: 

&nbsp;&nbsp;&nbsp;**1. Exploratory Data Analysis**  
&nbsp;&nbsp;&nbsp;**2. Defining Neighbors and Constructing a Weight Matrix**  
&nbsp;&nbsp;&nbsp;**3. Moran's I test for Spatial Autocorrelation**  
&nbsp;&nbsp;&nbsp;**4. Moran's I significance test through simulation**  
&nbsp;&nbsp;&nbsp;**5. Moran's scatter plot**   
&nbsp;&nbsp;&nbsp;**6. Correlogram**  
&nbsp;&nbsp;&nbsp;**7. Local Indicators of Spatial Autocorrelation (LISA)**  
&nbsp;&nbsp;&nbsp;**8. Ordinary Least Squares (OLS) Regression**  
&nbsp;&nbsp;&nbsp;**9. Geographically Weighted Regresssion (GWR)**   

<div class="alert alert-danger">
  <strong>REQUIRED!</strong> 
  
You are required to insert your outputs and any comment into this document. The document you submit should therefore contain the existing text in addition to:

 - Plots and other outputs from executing the code chunks
 - Discussion of your plots and other outputs as well as conclusions reached.
 - This should also include any hypotheses and assumptions made as well as factors that may affect your conclusions.
</div>

To help you  interpret and understand your results, please consult the following resources:

|||
|-|:---|
|**Chapter 10 - 12**|Introduction to geographic information systems by Kang-tsung (2011).              |
A copy of these are available on [Amathuba](https://amathuba.uct.ac.za/d2l/login?sessionExpired=0&target=%2fd2l%2fle%2flessons%2f30111%2ffolders%2f1057404) inside the `Resources` folder, Study Material & Books

----

```{r install }
options(prompt="> ", continue="+ ", digits=3, width=70,  show.signif.stars=F, repr.plot.width=7, repr.plot.height=7)
rm(list=ls())

# Install necessary packages: You only need to run this part once
##- install.packages(c("RColorBrewer", "classInt", "sp", sf", "ggplot2", "extrafont", "cowplot", spdep", "spgwr", "PerformanceAnalytics", "corrplot"))))

library(knitr)
library(RColorBrewer)  #color palettes
library(classInt) 
library(sp)
library(sf)  #simple features for spatial data
library(corrplot)
library(ggplot2)
library(extrafont)
library(cowplot)
library(corrplot)
library(Hmisc)
library(maptools)
library(VIM)
library(spdep)  #spatial regression
library(sfdep)  #spatial regression
library(spgwr)  #geographically weighted regression
library(PerformanceAnalytics)
#library(zoo)
```

```{r help }
#- help
help(package="spdep")
```


<div class="alert alert-success">
  <strong>THE DATASET:</strong> 

In this practical we will be working with South African Police Service (SAPS), Statistics South Africa (STATSSA) and the City of Cape Town (CoCT)  open data. The data set contains information on crime (combined residential burglaries and residential thefts) and property valuation as well as other variables specific to Cape Town, South Africa in 2015.  

<br>
    
**In the spirit of Open Science and Open Data the creation of this dataset is available as a seperate document: [latticeSpatial](https://github.com/AdrianKriger/r-lattice-data/blob/main/latticeSpatial.ipynb)**. It is open for inquiry, exploration and welcomes contribution, criticism and review.

<br>

Unit of analysis: 777 suburbs in Cape Town, South Africa


The variables contained in the dataset are:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`OBJECTID`   : polygon ID  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`OFC_SBRB_N` : official suburb name  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`SHAPE_Leng` : suburb perimeter  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`SHAPE_Area` : suburb area   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`OFFICIAL_SUBURB` : suburb name carried over from property valuation dataset  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`NUM_RES_PROP` : number residential properties sold.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`MED_LAND_EXTENT.m2` : median extent land (m^2)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`MED_TOT_BLD_AREA.m2` : median building area (m^2)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`GV2015VAL` : 2015 general property valuation  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`X` : x coordinate of suburb centroid  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`Y` : y coordinate of suburb centroid   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`AVESTADist` : average distance to the local police station  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`AVAPREArea` : average precinct area  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`AvePrecinctsLen` : average precinct perimeter  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`ROBRes` : residential robbery  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`BUGRes` : residential burglary  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`CRIME` : total theft  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`AVMEDINC` : average median household income  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`BACH` : number bachelors degrees  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`AVBACH` : average number bachelors degrees  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`STA` : police station  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`STAD` : distances to the local police station  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`PREA` : precinct area  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`PREL` : precinct perimeter  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`WARD` : official ward / track number  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚Ä¢	`geom` : suburb geometry

<br>

The dataset comes in the form of a `shapefile` within a `.geopackage` along with ALL the source material. Analysis will focus on `SHAPE_Area`, `GV2015_VAL`, `AVESTADist' 'AVAPREArea`, `CRIME`, `INCOME`, `AVBAC`.
 
</div>

<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 1.**  Why would we want to investigate the _**correlation**_ between the variables mentioned above? Who would use information related to the (spatial) relationship between crime, property valuation and income? 


<p class="comment">
[ Answer 1. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

____

## 1. Exploratory Data Analysis

```{r load-data }
coct = st_read('./SpatialStats_CoCT_HusCrm.gpkg', layer='LatticeDataR')
```

```{r crs }
st_crs(coct)
```


```{r class }
#- class of object
class(coct) # Class of object
```

```{r slot-names }
#- Check the Components of the SpatialPolygonsDataFrame
slotNames(coct) 
```

```{r names }
#- names of the columns in the dataset 
names(coct)
```

Notice the spatial aspect of the data; `geom` which represent the areas (each suburb) under study.

```{r structure}
#- for a deeper understanding of the dataset
str(coct)
```

<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 2.** What is the `espg` code of the coordinate reference system?

<p class="comment">
[ Answer 2. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

- **Question 3a.** Create a subset data from `coct` (select relevant variables and generate summary statistics etc.). Select the numerical variables in columns `4, 9, 12-to-14, 17, 18 and 20` and store the subset data into `coct_sub` variable. 

```{r answer3}
##-- click in this cell and type your answer here. your answer must be between the ``` back ticks ```

coct_sub <- coct[,c(4, 9, 12:13, 17:18, 20)]

```
- **Question 3b.** Use the `summary()` function to print the summary statistics?

```{r answer3b}
##-- click in this cell and type your answer here. your answer must be between the ``` backticks ```

summary(coct_sub)

```
- **Question 3c.** What is the `mean`, `maximum` and `minimum` variable for each data set?

<p class="comment">
[ Answer 3c. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

**Before we get to correlation lets first look at variation _within_ the variables of interest.** A quick way to do this is with a **histogram**. We add the `mean` (red) and `median` (blue) to help us understand the distribution 


```{r hist-crime }
hist(coct_sub$CRIME, 50, xlab = "Crime", main="")
abline(v=mean(coct_sub$CRIME, na.rm = TRUE), col="red")
abline(v=median(coct_sub$CRIME, na.rm = TRUE), col="blue")
```

```{r hist-income }
hist(coct_sub$AVMEDINC, 50, xlab = "Income", main="")
abline(v=mean(coct_sub$AVMEDINC), col="red")
abline(v=median(coct_sub$AVMEDINC), col="blue")
```

```{r hist-prop-val }
hist(coct_sub$GV2015VAL, 50, xlab = "Property Valuation", main="")
abline(v=mean(coct_sub$GV2015VAL, na.rm = TRUE), col="red")
abline(v=median(coct_sub$GV2015VAL, na.rm = TRUE), col="blue")
```

<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 4.** Comment on the `Crime`, `Income` and `Property Valuation`? Would you suggest `log` transforming the values? Your answer must be between 20 and 50 words per variable.

<p class="comment">
[ Answer 4. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

-----
#### Correlation

One of the goals of this exercise is to understand the **_relationship_** between a number of variables. To archive this aim we employ a `correlation matrix`, which is used to **investigate the dependence between multiple variables** at the same time. The result is a table containing the `correlation coefficients` between each variable and the others. 

<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 5.** Print and plot the correlation matrix and correlation plot and discuss the what you observe?

```{r correlation-matrix  }
# correlation 
coct_subdata <- st_drop_geometry(coct_sub)
cor(coct_subdata) 
```

```{r correlation-plot}
# Correlation plot using PerformanceAnalytic package
suppressWarnings({
chart.Correlation(coct_subdata, histogram=TRUE, pch=19)
})
```

</style>
<div class="alert alert-info"> <strong>HINT!</strong> You want to mention the _sign_ and _strength_ of the correlation </div> 

<p class="comment">
[ Answer 5. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

<div class="alert alert-warning"> <strong>TASK!</strong> </div>

- **Question 6.** Execute the next two `code chunks / blocks` and comment on what you observe.

```{r plot}
#- plot
plot(coct["CRIME"], lwd = 0.2)#, axes = TRUE)
```



```{r plot-binary}
# visualizing a variable - Binary classification
plot(coct['CRIME'], main = NA, #"City of Cape Town: Residential Crime, 2015 \n (cases per suburb)", 
     col = ifelse(coct$CRIME > 400, 
                                              "lightgrey", "red"))
title(main="City of Cape Town:\n Residential Crime, 2015 \n (cases per suburb)" , 
      adj = 0.9, line = 0.8)
# adding a legend to the graph
legend(x = "right", bty = 'n',
       #box.col = NA,
       #bg = NA, box.lwd = NA , 
       title="Res. Crime", legend=c("ResCRIME > 400", "ResCRIME < 400"), 
       fill = c("lightgrey", "red")
)
```


</style>
<div class="alert alert-info"> <strong>HINT! </strong> What conclusion/s can you draw about the data distribution in the previous three plots? Motivate your answer.

</div>

<p class="comment">
[ Answer 6. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

<br>

This is an **_excellent opportunity_** to highlight the challenges with using choropleth maps. Choropleth maps often give a false impression of an abrupt change at the boundaries of shaded units. These types of maps are also not suitable for showing total values.

These next series of plots illustrate three classification schemes. `Equal`, `Quantile` and `Jenks`.

<br>

<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 7.** Execute the following three `code blocks / chunks` and discuss the results.

```{r brewer }
# You could view your data using a more detailed classification
# Select Color scale - Create a Palette
pal = brewer.pal(7,"Greens") # Makes a 7-color spectral palette
display.brewer.pal(7, "Greens")  # This displays the colors
```

```{r break-intervals}
# classifying Data using classIntervals() 
# create class breaks 
brks.eq = classIntervals(coct$CRIME, n = 7, style = "equal")
brks.qt = classIntervals(coct$CRIME, n = 7, style = "quantile")
brks.jk = classIntervals(coct$CRIME, n = 7, style = "jenks")
# Other style options "fixed", "sd", "pretty", "kmeans", "hclust", "bclust" and "fisher"

# Link the color pallette to the class breaks (categories) using findColours(CATEGORIES, PALETTE)
brks.eqcol = findColours(brks.eq,pal)
brks.qtcol = findColours(brks.qt,pal)
brks.jkcol = findColours(brks.jk,pal)
```

```{r plot-equal}
# Plot with Equal Breaks
plot(coct['CRIME'],  col=brks.eqcol, lwd = 0.1, main=NA)
     #border=NA,  #"black")
legend("topleft", leglabs(round(brks.eq$brks, digits=1)), 
       fill=pal, cex=0.8, bty="n")
title(main="City of Cape Town: Residential Crime per suburb, 2015 \n (Equal breaks)")
```

```{r plot-quantile}
# Plot with Quantile Breaks
plot(coct['CRIME'],  col=brks.qtcol, lwd = 0.1, main=NA)
     #border=NA,  #"black") 
legend("topleft", leglabs(round(brks.eq$brks, digits=1)), 
       fill=pal, cex=0.8, bty="n")
title(main="City of Cape Town: Residential Crime per suburb, 2015 \n (Quantile breaks)")
```


```{r plot-jenks}
# Plot with Equal Breaks
plot(coct['CRIME'],  col=brks.jkcol, lwd = 0.1, main=NA)
     #border=NA,  #"black")
legend("topleft",leglabs(round(brks.jk$brks, digits=1)), 
       fill=pal, cex=0.8, bty="n")
title(main="City of Cape Town: Residential Crime per suburb, 2015 \n (Equal breaks)")
```


<p class="comment">
[ Answer 7. click in this cell and type your answer here. your answer must discuss all three classification techniques and be between the outer [] brackets  
- Equal:  
- Quantile:  
- Jenks
] </p>

____

## 2. Defining Neighbors and Constructing a Weight Matrix

In order to determine whether suburb attributes are _**spatially autocorrelated**_ or not, the **neighbours of each suburb need to be defined**. Each neighbor is then _given a weight that distinguishes it from the rest of the suburbs_ in the study area. 

By weighting the neighbors, the degree to which a given suburb attribute is spatially correlated with itself across the study area can then be quantified _(i.e. global spatial autocorrelation)_. 

In addition, the degree to which the neighbors‚Äô attributes are related to a given suburb or not can be computed _(i.e. local spatial autocorrelation)_.

We will only discuss one method of defining neighbors. **`Queens Weights`**. You are encouraged to explore **`Rook Weights`**.

<div class="alert alert-warning"> <strong>TASK!</strong> </div>

- **Question 8.** Execute the following three `code blocks / chunks` and discuss the results

```{r define-weight }
#- neighbourhood with queen's case:
coct.nb <- poly2nb(coct, queen = T)

#Define a network grid
coct.wts <- nb2listw(coct.nb, zero.policy = TRUE, style="W")
```

```{r summary-neigh}
summary(coct.nb)
```

```{r summary-weights }
# Explore the weight matrix
#summary(col.wts, zero.policy=TRUE)  
print(coct.wts, zero.policy=TRUE)  
```

```{r weight-plot }
##Visualize 

##Extract the Geometry
coords = st_coordinates(st_centroid(coct))

plot(coct['CRIME'], main = NA,
     col = "lightgray",
     border = "white", #fill = NA,
     reset = FALSE)
plot(coct.nb, coords,
     add = TRUE,
     col = 3,
     lwd = 2)
```

</style>
<div class="alert alert-info"> <strong>HINT!</strong> Comment on the average number of links and list the number of least and most connected regions
</div> 

<p class="comment">
[ Answer 8. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

<br>

<div class="alert alert-success">
  <strong>Why determine spatial autocorrelation?</strong> 

Spatial autocorrelation captures the degree to which near and distant things are related. We want to know how attributes (data at a location) are connected and their relationship. 

<br>
    
When we execute **_regression analysis_** we assume observations are _independent_. This is not always true. We can see this within the geospatial community where we typically describe a process as a function of location; i.e.: $Z(x,y)$. Consider the general form of an Ordinary Least Squares trend surface:

$$
    Z(x,y) = ùú∑_0 + ùú∑_1x + ùú∑_2y + œµ
$$
The locations $(x,y)$ are fixed and not random, but the attribute values $Z$ are assumed to have a random component. Our sample observations are not independent if they are spatially autocorrelated.

<br>
What this means is; if we where predicting elevation or crop yield or the prevalence of malaria the **_general trend_** could be captured with all the terms containing $ùú∑$ **_without accounting for a spatially autocorrelated random processes_**, such as localized topographic changes, clusters of different management practices or regional prevention and treatment campaigns.

When our observations are spatially autocorrelated our predictions will be false and we have to restructure our residuals ($œµ$) with a variogram. _(see [r-spatial-stats](https://adriankriger.github.io/r-spatial-stats/) next week)_
 
<br>
We test for spatial autocorrelation to know whether a variogram in necessary. 
</div>

____

## 3. Moran's I test for spatial autocorrelation

The most common method for testing _**spatial autocorrelation**_ is by using the `Global Moran‚Äôs I` test. This method **compares how similar each polygon is with its neighbours** and averages all these comparisons. The result is one value that will yield the overall impression of how a given variable is correlated with itself across the study area

|||
|-|:---|
|**`Null Hypothesis`**|There is no spatial autocorrelation for the CRIME variable.|
|**`Alt Hypothesis`**|There is spatial autocorrelation for the CRIME variable|

The easy way to calculate Moran‚Äôs I statistic is simply though the `moran.test` function. We execute the function with `zero.policy = TRUE` and `na.action = na.pass` to account for no-data values while `randomisation = T` tells us that the attribute being analyzed (crime in the City of Cape Town) is randomly distributed.

```{r global-moran }
m = length(coct$CRIME)
s = Szero(coct.wts)

globalMoran <- moran.test(coct$CRIME, coct.wts, zero.policy = TRUE,  
                     randomisation = T, 
                     na.action = na.pass)#, n = m, S0 = s) #
#globalMoran
```

```{r print-moran }
#- print
globalMoran
```

<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 8.** What conclusion can you draw from the `Moran I statistic` and the `p-value` for the `Global Moran's I`? Your answer **must include** comments on whether you accept or reject the the `Null or Alt Hypothesis`.

<p class="comment">
[ Answer 8. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

#### - Monte-Carlo method

The analytical approach to the `Moran‚Äôs I` benefits from being fast. But it may be sensitive to irregularly distributed polygons. A safer approach to _**hypothesis testing**_ is to run a Monte-Carlo (MC) simulation using the `moran.mc()` function. The `moran.mc()` takes an extra argument `n`, the number of simulations


```{r monte-carlo }

MC <- moran.mc(coct$CRIME, coct.wts, zero.policy = TRUE,  
                     nsim=999, #randomisation = T, 
                     #na.action = na.pass, 
               alternative ="greater")

# View results (including p-value)
MC
```
<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 9.** Does the `Monte-Carlo` simulation agree with the traditional `Moran's I`?

<p class="comment">
[ Answer 9. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

**We can `plot` the distribution of the `moran.mc()` methods Moran I values we expect had crime been randomly distributed across the suburbs** 

```{r plot-mc }
#- print
# Plot the Null distribution (nb: this is a density plot instead of a histogram)
plot(MC)
```

<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 10.** NOTICE: The observed `statistic = 0.8`  falls all the _way_ to the right. What does this mean? 

</style>
<div class="alert alert-info"> <strong>HINT!</strong> What do the positive and negative Moran statistic indicate?

</div> 

<p class="comment">
[ Answer 10. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

____

## 5. Moran‚Äôs scatter plot

We can visualize Morans statistic with a scatterplot. The plot places our variable of interest on the x-axis (crime) and the spatially weighted sum of neighbor values (i.e. spatially lagged values) on the y axis. We can think of the `Global Moran‚Äôs I` as a linear relationship between these (drawn as a slope in the figure).

```{r moran-scatter }
# moran Scatterplot

colsmp <- moran.plot(coct$CRIME, coct.wts, zero.policy = TRUE,
                     labels = as.character(coct.wts$OBJECTID),#OBJECTID), 
                     xlab = "CRIME", ylab = "Lag of CRIME")
```
<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 11.** What is the purpose of the `Global Moran's I` scatter plot? What does it represent? Discuss the `Global Moran's I` scatter plot above.

<p class="comment">
[ Answer 11. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

____

## 6. Correlogram

Another great way to visualize Morans statistic is a correlogram. A correlogram illustrates autocorrelation as a function of distance. As distance between suburbs (x-axis) increases, how does our spatial autocorrelation change (y-axis)?

```{r correl-plot }
#- correlogram
coct_cor8 <- sp.correlogram(coct.nb, coct$CRIME, order = 8, 
                           method = "I", zero.policy = TRUE)
#col_cor5 
#- plot correlogram
plot(coct_cor8, main = "Correlelogram of CRIME")
```
<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 12.** What conclusion/s can you draw from the correlogram plot? 

<p class="comment">
[ Answer 12. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

____

## 7. Local Indicators of Spatial Autocorrelation (LISA)

Like Global Moran‚Äôs I, the **Local Moran‚Äôs I statistic** can be used to _quantify spatial patterns occurring across a given area_. However, instead of producing one value that encompasses the entire study area, the **Local Moran‚Äôs I method produces a statistic for each polygon in the study area**.

The advantage of this localized method is that it shows exactly which polygons are similar or different to the objects in their neighborhood. In other words, it identifies **_where spatial autocorrelation occurs as well as the level of significance_** that is experienced.

```{r local-moran }
#- local Moran's I
coct_locm <- localmoran(coct$CRIME, coct.wts, zero.policy = TRUE,  
                     na.action = na.pass)
summary(coct_locm)
```

We get a number of useful statistics from the `localmoran()` model:

> Ii    : local moran statistic  
> E.Ii  : expectation of local moran statistic  
> Var.Ii: variance of local moran statistic  
> Z.Ii  : standard deviate of local moran statistic  
> Pr()  : p-value of local moran statistic

We can `plot()` these values to visualize how and the degree _(strength)_ of the connections / relationships. 

### - LISA clusters

LISA clusters identify localized regions where values are strongly positively or negatively associated with one another. We group suburbs with a `p-value` significance threshold of `0.05`

<div class="alert alert-warning"> <strong>TASK!</strong> </div>

- **Question 13.** Execute the following `code chunk / block` and discuss the result. 

```{r plot-local-moran-clusters }
quadrant <- vector(mode="numeric", length=nrow(coct_locm))

# centers the variable of interest around its mean
m.crime <- coct$CRIME - mean(coct$CRIME)     

# centers the local Moran's around the mean
m.local <- coct_locm[,1] - mean(coct_locm[,1])    

# significance threshold
signif <- 0.05 

# builds a data quadrant
quadrant[m.crime >0 & m.local>0] <- 4  
quadrant[m.crime <0 & m.local<0] <- 1      
quadrant[m.crime <0 & m.local>0] <- 2
quadrant[m.crime >0 & m.local<0] <- 3
quadrant[coct_locm[,5]>signif] <- 0   

# plot in r
brks <- c(0,1,2,3,4)
colors <- c("white", "blue", rgb(0,0,1, alpha=0.4), rgb(1,0,0,alpha=0.4), "red")
plot(coct['CRIME'], border="lightgray", main=NA,
     col=colors[findInterval(quadrant, brks, all.inside=FALSE)])
#box()
legend("bottomleft", legend =c("insignificant", "low-low", 
                               "low-high", "high-low", "high-high"),
       fill=colors, bty="n")
title(main="LISA clusters. City of Cape Town Crime (2015)")
```
</style>
<div class="alert alert-info"> <strong>HINT: </strong> What do the `low-low`, `low-high`, `high-low`, etc. mean?

</div>  

<p class="comment">
[ Answer 13. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

### -  `Local Moran` Plot

Similarly we can plot the Local Moran statistic (`Ii`) which indicates that the suburb is surrounded by units with similar values.

```{r plot-moran-Ii }
coct$coct_locm.Ii <- coct_locm[,1]
plot(coct['coct_locm.Ii'], main = 'Local Morans statistic (Ii)')#,#, 
```

```{r scale-crime }
CRIME <- scale(coct$CRIME) #%>% as.vector()
summary(CRIME)
```

```{r }
coct$lag_sCRIME <- lag.listw(coct.wts, CRIME, zero.policy = TRUE, NAOK = TRUE)
summary(coct$lag_sCRIME)
```
___

## 8. Ordinary Least Squares (OLS) Regression

Our aim is to examine the relationship between crime and suburb characteristics. 

Normally we take a dependent variable (crime) and try and explain the variation scores using an independent variable or a suite of uncorrelated and normally distributed independent variables (household income, property valuation, etc.). The strength and direction of association is indicated by the regression coefficients, with one coefficient given for each variable in the dataset.

Lets execute a basic OLS regression for crime (`CRIME`) per suburb on the following independent variables: Property Valuation (`GVAL2015`) and Income (`AVMEDINC`).

<div class="alert alert-warning"> <strong>TASK!</strong> </div>

- **Question 14.** Execute the following two `code chunks / blocks` and discuss the `summary()`. What conclusion can you draw from the `R-squared`, `Coefficients` and `p- value`s? Additionally comment on the _`sign`_ of the independent variables.

<div class="alert alert-block alert-success"> **REMEMBER WHAT WE ARE LOOKING FOR:**

A regression model gives us an idea of how different factors are associated with the target variable (`CRIME`) and their interaction.

<br>

**a. Significance.**  

|||
|-|:---|
|**`Null Hypothesis`**|The independent variable has NO significant influence / relationship to `CRIME` |
|**`Alt Hypothesis`**|The independent variable HAS significant influence / relationship to `CRIME` |

Smaller `p-value`s close-to 0 generally  indicate significance while 0 < `p-value`s > 0 typically indicate no significance.

<br>

**b. Coefficient Estimates.**
The `Estimate` indicates how the rate of change, for one unit of the predictor variable (`AVMEDINC`, `GV2015VAL`), are associated with the independent variable (`CRIME`)

<br>

**c. Model _fit._**
These are the `R-squared` values that allow us to understand the _**precision**_ of the model. Higher values are preferred. 
</div>

```{r ols-r }
# OLS Regression to predict CRIME using AVEMEDINC and GV2015VAL as independent variables

reg1 <- lm(coct$CRIME ~ coct$AVMEDINC + coct$GV2015VAL, na.action=na.exclude)
```

```{r ols-summary }
summary(reg1)
```
 

</style>
<div class="alert alert-info"> <strong>EXTRA! </strong> Your discussion should include why the `R-squared` is imprecise. How would the precision of the model become better? _(look at the summary. Its right there)_

</div>  

<p class="comment">
[ Answer 14. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>


<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 15.** `Run` the following `code chunk / block` which executes a `moran.test()` on the **_residuals_** of the OLS regression. Is there autocorrelation in the residuals? Substantiate your answer.

```{r ols-morans-residuals }
#- Moran's I on the residuals 
moran.test(residuals(reg1), coct.wts, alternative="two.sided", 
           zero.policy = TRUE,  na.action = na.exclude)
```

<p class="comment">
[ Answer 15. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

**Importantly we want to understand the residuals``

<div class="alert alert-warning"> <strong>TASK / QUESTION!</strong> </div>

- **Question 16.** Execute the following three `code chunks / blocks` which summaries the OLS residuals and generates a number of `plot()`s.

**a.**
```{r ols-residuals-summary }
#- ols residuals summary
summary(reg1$residuals)
```
**b.**

```{r  ols-stand-residuals }
#- ols standard residuals
s.resids <- rstudent(reg1)
plot(s.resids, type='h')
abline(h=c(-2, 2), col='lightskyblue')
```

**If we plot the standardized residuals plotted against the fitted values, we can see that as there is no clear pattern in the cloud.**

**c.**
```{r ols-regression-plot }
plot(reg1, which=3)
```
**d.**
```{r qqplot-ols-residuals}
library(car)
qqPlot(reg1)
```

</style>
<div class="alert alert-info"> <strong>HINT! </strong> Your answer should discuss outliers and the distribution

</div>

<p class="comment">
[ Answer 16. click in this cell and type your answer here. your answer must be between the outer [  
a. The summary of the residuals:  
b. The standard residuals:  
c. The standardized residuals vs the fitted values:  
d. the `qqPlot`:  
] brackets ]
</p>

**Similar to the LISA cluster; the `OLS residuals` better describe whether a spatial pattern exists or not** 

```{r }
# Plot the residuals
res <- residuals(reg1)
classes_sd <- classIntervals(res, n=4, style = "sd", rtimes = 1, dataPrecision = 3)
res.palette <- colorRampPalette(c("blue","white","red"), space = "rgb")
pal <- res.palette(4)
cols <- findColours(classes_sd,pal)

par(mar=rep(0,4))
plot(coct['CRIME'], col=cols, main="Residuals from OLS Model")#, border="grey")
legend("bottomright",cex=0.7,fill=attr(cols,"palette"),bty="n",
       legend=names(attr(cols, "table")),title="Residuals from OLS Model",ncol=4)

```

<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 14.** Elaborate on your answer from 13 above. Is there autocorrelation in the residuals? Substantiate your answer.

<p class="comment">
[ Answer 14. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

<div class="alert alert-block alert-success"> **Before we continue lets understand the purpose of regression analysis:** _we want to predict_ </div>


**Lets create a synthetic data set that represents an increase in $5\%$ household income (`AVMEDINC`) and an increase of $11\%$ in the value of properties (`GV2015VA`) then predict how `CRIME` will change.**

```{r }
test <- coct[,c(9, 18)]
test$AVMEDINC <- test$AVMEDINC * (100 + 5)/100
test$GV2015VAL <- na.approx(test$GV2015VAL,  rule = 2)
test$GV2015VAL <- test$GV2015VAL * (100 + 11)/100
test <- coct[,c(9, 18)]

# # create some test data 
# test = data.frame(
#   PctRural = c(10, 70, 50, 70, 10),
#   PctBach  = c(5, 5, 10, 10, 25),
#   PctEld   = c(2, 5, 5, 12, 15),
#   PctFB    = c(1, 1, 1, 5, 5),
#   PctPov   = c(20, 10, 8, 25, 25),  
#   PctBlack = c(5, 75, 20, 30, 75))

# predict using the model
predCrime = round(predict(reg1, newdata = test), 3)

test <- test %>% st_drop_geometry()
test$CRIME <- coct$CRIME

# display
data.frame(test, Predicted.Crime = predCrime)
```

<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 14.** Discuss the the result. Your answer should refer to the `Coefficients` and `p-values` of the OLS regression mode. Also discuss the empty rows.

<p class="comment">
[ Answer 14. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>


#### - Missing Values

<div class="alert alert-danger">
  <strong>WARNING!</strong> 
  
All the previous function accepted a `zero.policy = TRUE/FALSE` and  `na.action = na.exclude/pass` which allowed us to work around not having a complete data set (`NaN` and `NA` values). 

Geographically Weighted Regression (GWR) _**does not**_ accept `NaN` and `NA` values. 

In order to proceed onto GWR we either need to cull no-data rows or interpolate values. Since this exercise aims to demonstrate concepts and potential we interpolate. **This should not be standard practice.** 

</div>

**Lets once again work with a `subset` of the dataset.** And we do an extremely basic linear interpolation via `na.approx()`. First lets have a look at what is missing.

```{r mice }
#library(mice)


coct_sub <- coct[,c(4, 9:14, 17:18, 20, 26)]

summary(aggr(coct_sub))

```


**Looking at the _missingness_ pattern shows that there are 142 _missing_ values in total.** All 142 from `GVAL2015`  Moreover, there are 635 (81.$\%$) completely observed rows and 142 rows ($18.3\%$) with 1 _missings_.

**We `na.approx()` the data we need.**

```{r impute }
intp_coct <- coct_sub
intp_coct$GV2015VAL <- na.approx(coct_sub$GV2015VAL, rule = 2)
summary(aggr(intp_coct))
```

**Now we can continue**

___
## 9. Geographically Weighted Regresssion (GWR)

GWR is an outgrowth of OLS; and adds a level of modeling sophistication by allowing the relationships between the independent and dependent variables to vary by location.

Without getting into to much Math, since this is meant to be a brief introduction, the parameters in the GWR are estimated by weighted least squares and the role of this weight matrix is to **give more value to observations that are close to any one dependent variable**; as it is assumed that observations that are close will influence each other more than those that are far away _(Tobler‚Äôs Law)_.

Our three options with GWR are:  
- the kernel density function assigning weights;  
- the bandwidth `h` _---which determines the degree of distance decay---_ and  
- who to count as neighbors.

We start with a basic model and increase the complexity.

```{r gwr }

intp_coct.sp <- as(intp_coct, "Spatial")

# GWR with Gauss
crime.bw01 <- gwr.sel(intp_coct$CRIME ~ intp_coct$AVMEDINC + intp_coct$GV2015VAL, 
                  data=intp_coct.sp)# 
```

```{r gwr-bandwidth }
crime.bw01
```

This is basic estimated bandwidth (`h`); the distance (in meters, because our data are projected in a system measured in meters), which the weighting function will search, and include all observations within this radius. 

If we now `fit` with this search parameter.

```{r bw01-fit }
gwr.fit1 <- gwr(intp_coct$CRIME ~ intp_coct$AVMEDINC + intp_coct$GV2015VAL,
                data = intp_coct.sp, 
                bandwidth = crime.bw01, se.fit=T, hatmatrix=T)
```

```{r print-fit1 }
gwr.fit1
```
You are encouraged to change the weighting function to `bi-square` and `fit` another `regression` and / or add other variables (`AVESTADist`, `AVBACH`, etc.) to the regression to compare. We will however progress to the more robust solution.

#### - Adaptive kernel

The GWR models we ran above yielded a fixed distance to search for neighbors to include in the local regression. But there are places in our data where suburbs are more densely occurring. This means that in some areas, specifically along Voortrekker  Rd. heading east and Main Rd. heading south, we'll include a larger number of neighboring suburbs in the OLS compared to other areas, such as large areas on the periphery of the city‚Äôs boundaries.

In this case, an adaptive kernel is suitable.

```{r adapt }

crime.bw03 <- gwr.sel(intp_coct$CRIME ~ intp_coct$AVMEDINC + intp_coct$GV2015VAL, 
                  data=intp_coct.sp, 
                  adapt = TRUE)#, # 
```

```{r bw03 }

crime.bw03
```

Unlike the previous `h` this value is NOT a distance but the proportion of all cases which the weighting function will search, and include this fraction of observations in a model for each tract. The bandwidth distance will change according to the spatial density of features in the input feature class.

<div class="alert alert-warning"> <strong>TASK!</strong> </div>

- **Question 15.** Execute the following two `code chunks / blocks` and discuss the result.


```{r fit-adapt }
gwr.fit3 <- gwr(intp_coct$CRIME ~ intp_coct$AVMEDINC + intp_coct$GV2015VAL,
                data = intp_coct.sp, 
                adapt= crime.bw03, se.fit=T, hatmatrix=T)
```


```{r print-fit03 }
gwr.fit3
```

**We can see how `adapt` harvested different search distances across our study area** 

```{r print fit03.bw }
gwr.fit3$bandwidth
```
Or we could visualize the difference

```{r plot-gwr-adapt }
intp_coct$bwadapt <- gwr.fit3$bandwidth

plot(intp_coct['bwadapt'], #intp_col['bwadapt'], 
     main = 'GWR bandwidth')#,
```

<div class="alert alert-warning"> <strong>QUESTION!</strong> </div>

- **Question 15.** Discuss the result of `gwr.fit3$bandwidth` and the `plot()` above. Is the adaptive bandwidth more representative than a fixed distance for the entire study area?

<p class="comment">
[ Answer 15. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

The product of `gwr()` contains many wonderful parameters. The previous `code chunk / block` highlighted how the `adapt` option changed the bandwidth based on the size of the search area (suburb). **But what we really want to known is whether the coefficients are statistically significant _(is there a relationship between the variables? Is residential crime connected to property valuation and household income)_.** While these parameters are not readily available we can calculate them.

<div class="alert alert-warning"> <strong>TASK!</strong> </div>

- **Question 16.** Execute the following three `code chunks / blocks` and discuss the results.


```{r gwr-names }
names(gwr.fit3$SDF)
```

```{r }
#- get the degrees of freedom
dfree<-gwr.fit3$results$edf

#- calculate the t ratio
intp_coct$GV2015VAL.t <- gwr.fit3$SDF$intp_coct.GV2015VAL/gwr.fit3$SDF$intp_coct.GV2015VAL_se

#- calculate the pvalue using the pt()
intp_coct$GV2015VAL.t.p<-2*pt(-abs(intp_coct$GV2015VAL.t), dfree)
```

```{r plot-p }
plot(intp_coct['GV2015VAL.t.p'], #intp_col['bwadapt'], 
     main = 'p-value')#,#, border="lightgray", main=NA)
     #col = intp_col['bwadapt'])
```

</style>
<div class="alert alert-info"> <strong>HINT: </strong> What do the results suggest about the spatial heterogeneity of the relationship between the 2015 property valuation (`GVAL2015`) and residential theft? (`CRIME`)?</div>


<p class="comment">
[ Answer 16. click in this cell and type your answer here. your answer must be between the outer [] brackets ]
</p>

`spgwr` has a suite of tests comparing OLS and GWR models under an inferential framework. The null in these tests is the OLS and thus a statistically significant test statistic indicates that the GWR provides a _**statistically significant improvement**_ over an OLS in terms of its ability to match observed values. 

<div class="alert alert-warning"> <strong>TASK!</strong> </div>

- **Question 15.** Execute the following three `code chunks / blocks` that tests GWR and discuss the result.


```{r bfc }
BFC02.gwr.test(gwr.fit3)
```


```{r lmz-test }
LMZ.F1GWR.test(gwr.fit3)
```
Finally we can execute an Analysis of Variance (ANOVA) which will test the difference between the means of the OLS and GWR.

```{r anova }
anova(gwr.fit3)
```

</div>

<p class="comment">
[ Answer 15. click in this cell and type your answer here. your answer must be between the outer [] brackets with 50-to-150 words ]
</p>


<div class="alert alert-block alert-success"> **Now that we're done we can create a loverly interactive plot** </div>

```{r plotly-libraries }
library(plotly)
library(crosstalk)
```

```{r plotly }

#plot_ly() %>% config(scrollZoom = TRUE)

p <- plot_ly(
  coct, 
  split = ~OBJECTID,
  color = ~CRIME,
  colors = 'viridis',
  alpha = 1,
  showlegend = FALSE,
  #text = ~OFC_SBRB_N, 
  text = ~paste(round(CRIME), " residential theft cases in\n", OFC_SBRB_N),
  hoveron='fills',
  hoverinfo = "text"
  #alpha = 0.8) %>% 
  #layout(title = "Crude Rates"
         #config(scrollZoom = TRUE)
         ) %>% 
  config(scrollZoom = TRUE)

htmlwidgets::saveWidget(as_widget(p), "plotly.html")

p
```






